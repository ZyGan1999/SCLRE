<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" class="gr__shape2prog_csail_mit_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

              <meta name="viewport" content="width=device-width, initial-scale=1">    <link rel="shortcut icon" href="http://shape2prog.csail.mit.edu/images/favicon.ico">
        <meta name="description" content="Superclass Learning with Representation Enhancement">
        <meta name="keywords" content="Superclass Learning with Representation Enhancement">

        <title>Learning Superclass Representation via Semantic Reconstruction</title>
        <link rel="stylesheet" href="asset/font.css">
        <link rel="stylesheet" href="asset/main.css">
        <style>
            .abstract {
              border: 1px solid black; /* 边框样式和颜色 */
              padding: 10px; /* 可选：添加内边距以增加边框与内容之间的间距 */
              border-radius: 10px; /* 10像素的圆角边框 */
              background-color: #FEF9E7; /* 设置背景色为黄色 */
              margin-bottom: 60px; /* 在章节底部添加60像素的外边距 */
            }
            .chapter {
                border-radius: 10px;
                padding: 10px;
                background-color: #F6FEFF;
                margin-bottom: 60px; /* 在章节底部添加60像素的外边距 */
                box-shadow: 0 0 5px #2CACFF;

            }
            .urls {
                text-align: center; /* 水平居中容器中的内容 */
                font-size: 20px; /* 设置字体大小 */
                margin-bottom: 60px; /* 在章节底部添加60像素的外边距 */
            }
            .authors {
                text-align: center; /* 水平居中容器中的内容 */
                font-size: 20px; /* 设置字体大小 */
                margin-bottom: 60px; /* 在章节底部添加60像素的外边距 */
            }
            .box {
                border: 1px solid black; /* 边框样式和颜色 */
                border-radius: 10px; /* 10像素的圆角边框 */
                background-color: #FFF4F4; /* 设置背景色为黄色 */
                text-align: left; /* 水平居中容器中的内容 */
                margin-bottom: 30px; /* 在章节底部添加60像素的外边距 */
            }
            body {
                font-family: Arial, sans-serif;
            }
          </style>

    </head>

    <body data-gr-c-s-loaded="true">

        <div class="outercontainer">
            <div class="container">

                <div class="content project_title">
                    <h1><font size=6>Learning Superclass Representation via Semantic Reconstruction</font></h1>
                </div>

                <div class="authors">
                    Zeyu Gan, Suyun Zhao, Jinlong Kang, Hong Chen, Cuiping Li
                </div>

                <div class="urls">
                    <a href="https://github.com/ZyGan1999/Superclass-Learning-with-Representation-Enhancement">Code</a> | <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Superclass_Learning_With_Representation_Enhancement_CVPR_2023_paper.pdf">Conference Version</a>
                </div>


                


                <div class="abstract">
                    <div class="text">
                        <h1>Abstract</h1>
                        <p><font size=3>
                            This study explores the image representation learning problems with super coarse-grained annotations, i.e., superclass learning. 
                            A novel learning framework, SCLRE, is then proposed, which contributes to extracting superclass-aware representations by 
                            leveraging self-attention techniques across the instances and reconstructing the semantics. By theoretical analysis, we reveal 
                            that a well-aligned representation space is essential for superclass learning and the generalization error of SCLRE can be bounded 
                            by attention constraints. By empirical validation, extensive experiments verify that SCLRE outperforms the baseline and other 
                            contrastive-based methods on CIFAR-100 datasets and four high-resolution datasets.</font></p>
                    </div>
                </div>

                <div class="chapter">
                    <div class="text">
                        <h1>1. Introduction & Motivation</h1>
                        <div class="content project_headline">
                        <div class="img" style="text-align:center">
                            <img class="img_responsive" src="asset/intro.png" alt="Intro" style="margin:auto;max-width:80%">
                        </div>
                        <div class="text">
                            <p><font size=3>Figure 1: An illustration of superclass problem.</font></p>
                        </div>
                        <div class="text">
                        In real-world applications, the criteria for image classification are often determined by human cognition rather than the 
                        features of the images themselves. In some scenarios, due to overly coarse-grained criteria, a category of images may contain 
                        various subclasses, resulting in a lack of common semantic features even among images belonging to the same class. For example, 
                        in the context of waste classification, images of recyclable waste include a wide range of items, from beverage cans to books, 
                        with no apparent commonality. This article defines this phenomenon as the <b>Superclass Learning</b> problem.<br><br>
                        </div>
                        <div class="box">
                        <h3>&nbsp;Characteristics of Superclass Learning</h3>
                        <p><ul class="download">
							<font size=3>
                            <li>Subclasses within a superclass are usually scattered and share few common features.</li>
							<li>Instances from different superclasses may have common features, leading to potential confusion between classes.</li>
                            </font>
                        </ul></p>
                        </div>

                        <div class="box">
                        <h3>&nbsp;Challenges of Superclass Learning</h3>
                        <p><ul class="download">
							<font size=3>
                            <li><b>Breaking the original basic class decision boundaries</b>(Fig 1.b): It is necessary to divide the domain into smaller, more meaningful domains that better represent the actual problem (e.g., splitting the apple domain into fruit and toy domains).</li>
							<li><b>Reconstructing decision boundaries at the superclass level</b>(Fig 1.c): The goal is to merge relevant class domains into a new superclass domain that accurately represents the intended classification (e.g., combining fruit apples, eggs, and bones into a food waste superclass domain).</li>
                            </font>
                        </ul></p>
                        </div>

                        <div class="box">
                            <h3>&nbsp;Contribution of this paper</h3>
                        <p><ul class="download">
							<font size=3>
                            <li>We propose the under-study but realistic problem, superclass learning.</li>
							<li>We propose a novel representation enhancement method (SCLRE) to address superclass learning.</li>
                            <li>We perform extensive experiments to demonstrate that SCLRE outperforms other classification techniques.</li>
                            <li>We conduct extensive analysis about the isotropy of superclass learning and extend SCLRE into semi-supervised situations.</li>
                        </font>
                        </ul></p>
                        </div>
                    </div>
                    </div>
                </div>
                <div class="chapter">
                    <div class="text">
                        <h1>2. Framework of SCLRE</h1>
                    </div>

                    <div class="content project_headline">
					
                        <div class="img" style="text-align:center">
                            <img class="img_responsive" src="asset/model.png" alt="result" style="margin:auto;max-width:90%">
                        </div>
                        <div class="text">
                            <p><font size=3>Figure 2: Overview of the process of SCLRE.</font></p>
                        </div>
                        <div class="text">
                            SCLRE processes the images in the following steps. The images first generate their representations 
                            through a convolutional neural network, then mix with each other in a trainable cross-instance 
                            attention module for enhancement. After enhancement, the representations are then adjusted according 
                            to their superclass labels and the target anchors.
                        </div>

                        <div class="content">
                            <h3>Enhancement</h3>
                        <p><ul class="download">
							<font size=3>
                            <li>We calculate the attention weights by putting the batch images into a cross-instance attention module.</li>
							<li>We use the attention weights to mix the representations with each other.</li>
                            </font>
                        </ul></p>
                        </div>

                        <div class="content">
                            <h3>Adjustment</h3>
                        <p><ul class="download">
							<font size=3>
                            <li>We adjust the enhanced representations under the guidance of a supervised contrastive loss.</li>
							<li>We also preset target anchors for each superclass to help the adjustment process.</li>
                            </font>
                        </ul></p>
                        More details about the enhancement and adjustment process are shown in our paper (Sec.4).
                        </div>
                        
                    </div>
					
                </div>
				<div class="chapter">
                    <div class="text">
                        <h1>3. Experiment Result</h1>

						<h2> 3.1 Accuracy </h2>

                        <p style="text-align: center;"><font size=3>Table 1&2: Accuracy on Multiple Datasets.</font></p>
						<div class="img" style="text-align:center">
                            <img class="img_responsive" src="asset/accuracy.png" alt="result" style="margin:auto;max-width:90%">
                        </div>
                        <div class="text">
                            We reorganize the raw classes of these datasets and make them superclass problems. The classification 
                            results show that SCLRE outperforms the SOTA representation learning framework.
                        </div>

                        <h2> 3.2 Visualization </h2>
					
                        <center>
                            <img class="img_responsive" src="asset/visualization.png" alt="result" style="margin:auto;max-width:90%">
                            <div class="text">
                                <p><font size=3>Figure 3: Visualization of Representation Space.</font></p>
                            </div>
                        </center>

                        <div class="text">
                            We use t-SNE to visualize the representation space of each compared method. The result shows that SCLRE can 
                            effectively extract superclass-aware representations and make the boundaries of each superclass more clear.
                        
                            More experiment results like ablation, sensitivity, and robustness are shown in our paper.
                        </div>
                    </div>
                </div>

                <div class="chapter">
                    <div class="text">
                    <h1>4. Analysis of Generalization</h1>
                    According to the theory of contrastive learning, we analyze the generalization upper bound of SCLRE. The details of the proof 
                    are shown in the paper and appendix. To draw a conclusion, The generalization error can be bounded by the similarity of attention 
                    vectors from the same superclass in the form of the following equation:
                    </div>

                    <center>
                        <img class="img_responsive" src="asset/error.png" alt="result" style="margin:auto;max-width:50%">
                    </center>
                    <div class="text">
                        During the training process, for the samples of the same superclass under the same task, the SCLRE training process 
                        can help to discover samples that are more important to the construction of the superclass structure, and they should 
                        also be similar to the samples in the same superclass. Therefore, the sample pairs in O2 can naturally have high 
                        similarity attention vectors, thereby reducing the upper bound of the generalization error, which is consistent with 
                        the phenomenon we observed in the experiment.
                    </div>
                </div>

               
                

            <div class="chapter">
                <h1>5. Extension to Semi-Supervised Learning</h1>
                We further extend SCLRE to semi-supervised learning. By applying self-supervised pretraining and a strict threshold,
                we get reliable new generated pseudo-labels. Then we use the pseudo-labels to train the model in a semi-supervised way. 
                For convenience, we call this method SS-SCLRE. The experiment results show that SS-SCLRE can achieve better performance and alleviate performance decay.
                <center>
                    <img class="img_responsive" src="asset/semi_line.png" alt="result" style="margin:auto;max-width:80%">
                    <p><font size=3>Figure 4: Results about the Semi-Supervised Extension.</font></p>
                </center>

                The details of the extension are shown in the paper.
            </div>



            <div class="chapter">
                <h1>BibTex  </h1>
                    <b>Recommended citation</b>(for the conference version): <br>
                    Zeyu Gan, Suyun Zhao, Jinlong Kang, Liyuan Shang, Hong Chen, and Cuiping Li. Superclass learning with representation enhancement. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pages 24060-24069, June 2023.
        </div>

</body></html>


